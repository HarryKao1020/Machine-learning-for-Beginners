# Perceptron感知器 - 線性分類



#### Perceptron介紹
以生物的**神經元模型** 為基礎開發的演算法
左邊的突觸是神經接收訊息的地方(input)，若input的訊號總和強度大於等於某一個值(threshold)，這個神經元就會透過軸突(axon)發送訊息到右邊的突觸給下一個神經元。舉例來說當你的膝蓋被打超過一定的力道時，膝蓋神經就會發出訊號給肌肉內的神經進行所謂的膝反射。

假設左邊的突觸有n個，每一個input以 x1, x2, x3,…xn來表示的話，當w1*x1 + w2*x2 + w3*x3 +…. wn*xn >某一個定值，就會出發神經元發送信號出去。w1,w2,…wn,以及某一個定值都是這個神經元根據過往經驗學會的數字，也是資料科學家平常說在train這個model到底是在train什麼，就是找出w1,w2…wn以及某一個定值。

--- 

#### 資料分類的前提
Perceptron這個演算法只有在資料是線性可分的形況下才能正確分類（演算法才會停止）。什麼是資料是線性可分呢？以2D的情況簡單來說就是可以在平面上找一條線去完全切出這兩群，3D的話就是可以在空間中找一個平面去切出兩群

| 相關網站:https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-2%E8%AC%9B-%E7%B7%9A%E6%80%A7%E5%88%86%E9%A1%9E-%E6%84%9F%E7%9F%A5%E5%99%A8-perceptron-%E4%BB%8B%E7%B4%B9-84d8b809f866


---

### 內積(點積) 在perceptron的用途
內積（Dot Product）在感知機（Perceptron）中扮演著**核心且關鍵**的角色，它是模型用來做出**分類決策**的最重要運算。

簡單來說，內積的作用是計算「輸入資料」與「模型學習到的權重」之間的**相似度或匹配程度**。這個計算結果決定了感知機最終的分類結果。

---

### 核心概念：權重與輸入的加權總和

感知機是一種線性分類器，它的目標是找到一條直線或超平面，來將不同類別的資料點分開。內積正是用來定義這條直線或超平面的方程式。

你可以將內積想像成一個**加權投票的過程**：

* **權重向量 ($\mathbf{w}$)**：代表模型學到的「判斷標準」。向量中的每個權重 $w_i$ 代表了對應輸入特徵 $x_i$ 的重要性。權重越大，該特徵對最終決策的影響力就越大。
* **輸入向量 ($\mathbf{x}$)**：代表你要分類的資料點，其中每個 $x_i$ 都是一個特徵值。
* **內積 ($\mathbf{w} \cdot \mathbf{x}$)**：將每個特徵 $x_i$ 與其對應的重要性 $w_i$ 相乘，然後將所有結果加總。這就是一個**加權總和**，這個總和越高，代表這個輸入資料與模型學到的「正面類別」標準越吻合。

### 數學表達式

感知機的決策函數可以寫成：

$$z = \mathbf{w} \cdot \mathbf{x} + b = \sum_{i=1}^{n} w_i x_i + b$$

* **$\mathbf{w}$**：權重向量
* **$\mathbf{x}$**：輸入向量
* **$b$**：偏置（Bias）
* **$z$**：內積加偏置後的結果

這個 $z$ 值是感知機的**決策分數**，其正負號決定了分類結果：

* 如果 $z > 0$：感知機判斷該資料點屬於**正類（例如：+1）**。
* 如果 $z \le 0$：感知機判斷該資料點屬於**負類（例如：-1）**。

### 內積在幾何上的意義

從幾何學角度來看，內積 $\mathbf{w} \cdot \mathbf{x}$ 衡量了向量 $\mathbf{w}$ 和 $\mathbf{x}$ 的**方向一致性**。

* 當 $\mathbf{w}$ 和 $\mathbf{x}$ 方向相似時，內積值為正且較大。
* 當 $\mathbf{w}$ 和 $\mathbf{x}$ 方向相反時，內積值為負且較大。
* 當 $\mathbf{w}$ 和 $\mathbf{x}$ 互相垂直時，內積值為零。

這意味著，感知機的**權重向量 $\mathbf{w}$ 其實就是垂直於分類超平面的法向量**。內積就是判斷輸入資料點 $\mathbf{x}$ 是位於超平面的哪一側。

---

### 舉例說明

假設我們要用感知機判斷一個客戶是否會購買產品。

* **輸入 $\mathbf{x}$**：包含「瀏覽時間」、「過去的購買金額」等特徵。
* **權重 $\mathbf{w}$**：模型學習到「瀏覽時間」的權重為 0.2，「購買金額」的權重為 0.8。

當一個新客戶的「瀏覽時間」為 10 分鐘，「過去購買金額」為 500 元時，內積計算如下：

$$內積分數 = (0.2 \times 10) + (0.8 \times 500) = 2 + 400 = 402$$

這個 402 就是加權總和。如果我們設定的偏置 $b$ 是 -100，則 $z = 402 - 100 = 302 > 0$，感知機就會判斷這個客戶會購買產品。

因此，內積在感知機中，不僅僅是一個數學運算，它更是**權重與特徵結合的決策機制**，是模型做出線性分類判斷的根本。

---

### z大於0,小於0,等於0的用處？
這是一個非常核心且重要的問題，為什麼是「大於0」和「小於0」這個簡單的二分法，這背後其實有著深刻的數學和幾何意義。



### 1. 幾何上的意義：超平面

感知機的決策是基於一個**超平面 (Hyperplane)**。在二維空間中，這個超平面就是一條直線；在三維空間中，它是一個平面。這個超平面將整個空間分成兩個區域。

* **$\mathbf{w} \cdot \mathbf{x} + b = 0$** 這條方程式定義了這個超平面。
* **$\mathbf{w} \cdot \mathbf{x} + b > 0$** 代表超平面**某一側**的區域。
* **$\mathbf{w} \cdot \mathbf{x} + b < 0$** 代表超平面**另一側**的區域。

所以，感知機的分類決策，其實就是在判斷一個輸入點 $\mathbf{x}$ 是位於這個超平面的哪一側。

---

### 2. 數學上的意義：法向量與距離

你可能還記得，在線性代數中，向量 $\mathbf{w}$ 其實是**垂直於**超平面的**法向量 (Normal Vector)**。

* 內積 $\mathbf{w} \cdot \mathbf{x}$ 的結果，可以看作是將向量 $\mathbf{x}$ 投影到向量 $\mathbf{w}$ 上的長度。
* **$z = \mathbf{w} \cdot \mathbf{x} + b$** 這個值其實就代表了資料點 $\mathbf{x}$ 到超平面的**有號距離 (Signed Distance)**，也就是說：
    * 如果 **$z > 0$**，表示資料點 $\mathbf{x}$ 位於超平面的法向量 $\mathbf{w}$ 所指向的那一側。
    * 如果 **$z < 0$**，表示資料點 $\mathbf{x}$ 位於超平面的另一側。
    * 如果 **$z = 0$**，表示資料點 $\mathbf{x}$ **正好在超平面上**。

因此，「大於0」和「小於0」並非隨機選擇，它們直接反映了資料點相對於決策超平面的位置。

### 3. 生物上的意義：神經元的激發

感知機的設計靈感來自於生物神經元。在神經學中，神經元接收到足夠的刺激（也就是**加權總和**超過某個**閾值**）時，才會被「激發」，並傳遞訊號。

* **加權總和 $\mathbf{w} \cdot \mathbf{x}$** 就是神經元接收到的刺激強度。
* **偏置 $b$** 則可以被看作是這個閾值的負值。
* 當**刺激強度**超過**閾值**時，即 **$\mathbf{w} \cdot \mathbf{x} > -b$**，也就是 **$\mathbf{w} \cdot \mathbf{x} + b > 0$** 時，神經元就會被激發，並輸出一個正的訊號（例如 +1）。

這種「閾值激發」的機制，完美地對應了「大於0」和「小於0」的分類邏輯。

---

總結來說，感知機之所以採用「大於0」和「小於0」的判斷，是因為這兩個區域在幾何、數學和生物學上都有清晰的意義。這是一個簡單而優雅的方法，將一個複雜的分類問題，簡化成了判斷資料點位於超平面的哪一側。
